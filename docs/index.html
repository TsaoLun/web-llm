<!DOCTYPE html>
<html>
    <head>
        <title>WebLLM | Home</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <!-- <link rel="stylesheet" href="/web-llm/css/table.css">          -->
        <meta http-equiv="origin-trial" content="Agx76XA0ITxMPF0Z8rbbcMllwuxsyp9qdtQaXlLqu1JUrdHB6FPonuyIKJ3CsBREUkeioJck4nn3KO0c0kkwqAMAAABJeyJvcmlnaW4iOiJodHRwOi8vbG9jYWxob3N0Ojg4ODgiLCJmZWF0dXJlIjoiV2ViR1BVIiwiZXhwaXJ5IjoxNjkxNzExOTk5fQ==">
<meta http-equiv="origin-trial" content="AnmwqQ1dtYDQTYkZ5iMtHdINCaxjE94uWQBKp2yOz1wPTcjSRtOHUGQG+r2BxsEuM0qhxTVnuTjyh31HgTeA8gsAAABZeyJvcmlnaW4iOiJodHRwczovL21sYy5haTo0NDMiLCJmZWF0dXJlIjoiV2ViR1BVIiwiZXhwaXJ5IjoxNjkxNzExOTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZX0=">
<script src="dist/tvmjs_runtime.wasi.js"></script>
<script src="dist/tvmjs.bundle.js"></script>

    </head>
    <body>
        <div class="container">
            <!-- This is a bit nasty, but it basically says be a column first, and on larger screens be a spaced out row -->
     

            

            
            <!-- Schedule  -->

            <script>
              var tvmjsGlobalEnv = tvmjsGlobalEnv || {};
            </script>
            
            <script type="module">
              async function getTokenizer(url) {
                const mod = await import("./dist/sentencepiece/index.js");
                return await mod.sentencePieceProcessor(url);
              }
              tvmjsGlobalEnv.sentencePieceProcessor = getTokenizer;
            </script>
            
            <script>
              function handleChatUIInputEnter(event) {
                if (event.keyCode === 13) {
                  tvmjsGlobalEnv.asyncOnGenerate();
                }
              }
              async function getTokenizer(url) {
                const mod = await import("./dist/sentencepiece/index.js");
                return await mod.sentencePieceProcessor(url);
              }
              tvmjsGlobalEnv.sentencePieceProcessor = getTokenizer;
            </script>
            
            <script src="dist/llm_chat.js"></script>
            
            <link href="dist/llm_chat.css" rel="stylesheet" type="text/css" />
            
            <div class="chatui">
              <div class="chatui-chat" id="chatui-chat" height="100">
              </div>
            
              <div class="chatui-inputarea">
                <input id="chatui-input" type="text" class="chatui-input" onkeypress="handleChatUIInputEnter(event)" placeholder="Enter your message..." />
                <button class="chatui-send-btn" onclick="tvmjsGlobalEnv.asyncOnGenerate()">Send</button>
              </div>
            </div>
            
            <div class="chatui-extra-control">
              <button class="chatui-reset-btn" onclick="tvmjsGlobalEnv.asyncOnReset()">Reset</button>
              <label id="chatui-info-label"></label>
            </div>
            
                <h1 id="web-llm">Web LLM</h1>

<p>This project brings large-language model and LLM-based chatbot to web browsers. <strong>Everything runs inside the browser with no server support and accelerated with WebGPU.</strong> This opens up a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration. Please check out our <a href="https://github.com/mlc-ai/web-llm">GitHub repo</a> to see how we did it. There is also a <a href="#chat-demo">demo</a> which you can try out.</p>

<h2 id="instructions">Instructions</h2>

<p>WebGPU just shipped to Chrome and is in beta. We do our experiments in <a href="https://www.google.com/chrome/canary/">Chrome Canary</a>.  You can also try out the latest Chrome 113. Chrome version ≤ 112 is not supported, and if you are using it, the demo will raise an error like <code class="language-plaintext highlighter-rouge">Find an error initializing the WebGPU device OperationError: Required limit (1073741824) is greater than the supported limit (268435456). - While validating maxBufferSize - While validating required limits.</code>
We have tested it on windows and mac, you will need a gpu with about 6.4G memory.</p>

<p>If you have a Mac computer with Apple silicon, here are the instructions for you to run the chatbot demo on your browser locally:</p>

<ul>
  <li>Install <a href="https://www.google.com/chrome/canary/">Chrome Canary</a>, a developer version of Chrome that enables the use of WebGPU.</li>
  <li>Launch Chrome Canary. You are recommended to launch from terminal with the following command (or replace Chrome Canary with Chrome):
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Applications/Google\ Chrome\ Canary.app/Contents/MacOS/Google\ Chrome\ Canary --enable-dawn-features=disable_robustness
</code></pre></div>    </div>
    <p>This command turns off the robustness check from Chrome Canary that slows down image generation to times. It is not necessary, but we strongly recommend you to start Chrome with this command.</p>
  </li>
  <li>Enter your inputs, click “Send” – we are ready to go! The chat bot will first fetch model parameters into local cache. The download may take a few minutes, only for the first run. The subsequent refreshes and runs will be faster.</li>
</ul>

<p>The chat model is based on <a href="https://huggingface.co/lmsys/vicuna-7b-delta-v0">vicuna-7b-v0</a> model. More model support are on the way.</p>

<h2 id="links">Links</h2>

<ul>
  <li><a href="https://github.com/mlc-ai/web-llm">Web LLM Github</a></li>
  <li>You might also be interested in <a href="https://mlc.ai/web-stable-diffusion/">Web Stable Diffusion</a>.</li>
</ul>

<h2 id="disclaimer">Disclaimer</h2>

<p>This demo site is for research purposes only, subject to the model License of LLaMA and Vicuna. Please contact us if you find any potential violation.</p>

            
        </div> <!-- /container -->

        <!-- Support retina images. -->
    </body>

</html>
